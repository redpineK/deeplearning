{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0OQNmJ4/0i2W086VfCjYa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/redpineK/deeplearning/blob/master/vanila_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/PacktPublishing/Deep-Learning-with-TensorFlow-2-and-Keras/blob/master/Chapter%206/VanillaGAN.ipynb"
      ],
      "metadata": {
        "id": "oUa3H313h0LG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1kp2igcLhu5v"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import initializers\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seed for reproducible result\n",
        "np.random.seed(1000)\n",
        "\n",
        "randomDim = 10 \n",
        "# Load MNIST data\n",
        "(X_train, _), (_, _) = mnist.load_data()\n",
        "X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
        "X_train = X_train.reshape(60000, 784)"
      ],
      "metadata": {
        "id": "zzyIhFMBh43s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "adam = Adam(lr=0.0002, beta_1=0.5)\n",
        "\n",
        "generator = Sequential()\n",
        "generator.add(Dense(256, input_dim=randomDim)) #, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "generator.add(LeakyReLU(0.2))\n",
        "generator.add(Dense(512))\n",
        "generator.add(LeakyReLU(0.2))\n",
        "generator.add(Dense(1024))\n",
        "generator.add(LeakyReLU(0.2))\n",
        "generator.add(Dense(784, activation='tanh'))\n",
        "#generator.compile(loss='binary_crossentropy', optimizer=adam)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqZCE3c4h_vW",
        "outputId": "23a2f706-efb5-4640-f4ec-492e0c702724"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = Sequential()\n",
        "discriminator.add(Dense(1024, input_dim=784, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "discriminator.add(LeakyReLU(0.2))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Dense(512))\n",
        "discriminator.add(LeakyReLU(0.2))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Dense(256))\n",
        "discriminator.add(LeakyReLU(0.2))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Dense(1, activation='sigmoid'))\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=adam)"
      ],
      "metadata": {
        "id": "SjGW4bHiiDOq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined network\n",
        "discriminator.trainable = False\n",
        "ganInput = Input(shape=(randomDim,))\n",
        "x = generator(ganInput)\n",
        "ganOutput = discriminator(x)\n",
        "gan = Model(inputs=ganInput, outputs=ganOutput)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=adam)\n",
        "\n",
        "dLosses = []\n",
        "gLosses = []"
      ],
      "metadata": {
        "id": "_1smnCtAiGce"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss from each batch\n",
        "def plotLoss(epoch):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(dLosses, label='Discriminitive loss')\n",
        "    plt.plot(gLosses, label='Generative loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('images/gan_loss_epoch_%d.png' % epoch)\n",
        "\n",
        "# Create a wall of generated MNIST images\n",
        "def saveGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):\n",
        "    noise = np.random.normal(0, 1, size=[examples, randomDim])\n",
        "    generatedImages = generator.predict(noise)\n",
        "    generatedImages = generatedImages.reshape(examples, 28, 28)\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generatedImages.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(generatedImages[i], interpolation='nearest', cmap='gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('images/gan_generated_image_epoch_%d.png' % epoch)"
      ],
      "metadata": {
        "id": "9YWnC4LfiPmn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs=1, batchSize=128):\n",
        "    batchCount = int(X_train.shape[0] / batchSize)\n",
        "    print ('Epochs:', epochs)\n",
        "    print ('Batch size:', batchSize)\n",
        "    print ('Batches per epoch:', batchCount)\n",
        "\n",
        "    for e in range(1, epochs+1):\n",
        "        print ('-'*15, 'Epoch %d' % e, '-'*15)\n",
        "        for _ in range(batchCount):\n",
        "            # Get a random set of input noise and images\n",
        "            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n",
        "            imageBatch = X_train[np.random.randint(0, X_train.shape[0], size=batchSize)]\n",
        "\n",
        "            # Generate fake MNIST images\n",
        "            generatedImages = generator.predict(noise)\n",
        "            # print np.shape(imageBatch), np.shape(generatedImages)\n",
        "            X = np.concatenate([imageBatch, generatedImages])\n",
        "\n",
        "            # Labels for generated and real data\n",
        "            yDis = np.zeros(2*batchSize)\n",
        "            # One-sided label smoothing\n",
        "            yDis[:batchSize] = 0.9\n",
        "\n",
        "            # Train discriminator\n",
        "            discriminator.trainable = True\n",
        "            dloss = discriminator.train_on_batch(X, yDis)\n",
        "\n",
        "            # Train generator\n",
        "            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n",
        "            yGen = np.ones(batchSize)\n",
        "            discriminator.trainable = False\n",
        "            gloss = gan.train_on_batch(noise, yGen)\n",
        "\n",
        "        # Store loss of most recent batch from this epoch\n",
        "        dLosses.append(dloss)\n",
        "        gLosses.append(gloss)\n",
        "\n",
        "        if e == 1 or e % 20 == 0:\n",
        "            saveGeneratedImages(e)\n",
        "            \n",
        "\n",
        "    # Plot losses from every epoch\n",
        "    plotLoss(e)\n"
      ],
      "metadata": {
        "id": "bzAFD_o_iVtX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = './images/'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)"
      ],
      "metadata": {
        "id": "owt3bemxjkIa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(200, 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSlTLCPbiYNW",
        "outputId": "675bf114-f8c4-4f77-f6e0-edac1884c251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 200\n",
            "Batch size: 128\n",
            "Batches per epoch: 468\n",
            "--------------- Epoch 1 ---------------\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n"
          ]
        }
      ]
    }
  ]
}