{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwTyybDZgho+H3gyIUGfgF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/redpineK/deeplearning/blob/master/RNN%EA%B3%B5%EB%B6%80%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://nicola-ml.tistory.com/128?category=874784"
      ],
      "metadata": {
        "id": "2DREmV_Jqitu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "    \n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "#가장 빈도가 높은 1,000개의 단어만 선택하도록 Tokenizer객체를 만듭니다.\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "#단어 인덱스를 구축합니다.\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "#문자열을 정수 인덱스의 리스트로 변환합니다.\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "\n",
        "#직접 원-핫 이진 벡터 표현을 얻을 수 있습니다. 원-핫 인코딩 외에 다른 벡터화 방법들도 제공합니다.\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "\n",
        "#계산된 단어 인덱스를 구합니다.\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print(len(word_index))\n",
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM_XCze6pgRm",
        "outputId": "ab84945d-4925-4a80-b6c3-fc2e1c603270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "\n",
        "#특성으로 사용할 단어의 수\n",
        "max_features = 10000\n",
        "#사용할 텍스트의 길이(가장 빈번한 max_features개의 단어만 사용합니다.)\n",
        "maxlen = 20\n",
        "\n",
        "#정수 리스ㅡ로 데이터를 로드합니다.\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words =  max_features)\n",
        "\n",
        "#리스트를(samples, maxlen)크기의 2D 정수 텐서로 변환합니다. \n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_kdwuU8p7iN",
        "outputId": "10a3a950-c7b6-4585-9f1a-88b9b9a009ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Embedding\n",
        "\n",
        "model = Sequential()\n",
        "#나중에 임베딩된 입력을 Flatten 층에서 펼치기 위해 Embeding층에 input_length를 지정합니다.\n",
        "#Embedding 층의 출력 크기는 (samples, maxlen, 8)이 됩니다.\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "\n",
        "#3D 임베딩 텐서를 (samples, maxlen*8) 크기의 2D 텐서로 펼칩니다.\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "test_mse_score, test_mae_score = model.evaluate(x_test, y_test)\n",
        "print(test_mse_score, test_mae_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHowELeiqVW0",
        "outputId": "8e31b6f9-4302-49e1-999f-ab866654d152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 20, 8)             80000     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 160)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 161       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.6643 - acc: 0.6309 - val_loss: 0.6066 - val_acc: 0.6950\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.5300 - acc: 0.7544 - val_loss: 0.5198 - val_acc: 0.7352\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.4562 - acc: 0.7901 - val_loss: 0.4983 - val_acc: 0.7486\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.4204 - acc: 0.8086 - val_loss: 0.4932 - val_acc: 0.7524\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3959 - acc: 0.8231 - val_loss: 0.4927 - val_acc: 0.7586\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3761 - acc: 0.8331 - val_loss: 0.4971 - val_acc: 0.7598\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3583 - acc: 0.8440 - val_loss: 0.5022 - val_acc: 0.7572\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3415 - acc: 0.8536 - val_loss: 0.5096 - val_acc: 0.7532\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3253 - acc: 0.8625 - val_loss: 0.5145 - val_acc: 0.7560\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3094 - acc: 0.8716 - val_loss: 0.5248 - val_acc: 0.7520\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5103 - acc: 0.7612\n",
            "0.5103055238723755 0.7611600160598755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "#100개 단어 이후는 버립니다.\n",
        "maxlen = 100\n",
        "\n",
        "#훈련 샘플은 200개입니다.\n",
        "training_samples = 200\n",
        "\n",
        "#검증 샘플은 1만 개입니다.\n",
        "validation_samples = 10000\n",
        "\n",
        "#데이터셋에서 가장 빈도 높은 1만 개의 단어만 사용합니다.\n",
        "max_words = 10000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = np.asarray(labels)\n",
        "print('데이터 텐서의 크기:', data.shape)\n",
        "print('레이블 텐서의 크기:', labels.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = data[training_samples: training_samples + validation_samples]"
      ],
      "metadata": {
        "id": "lXxNQdK8qp1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frI877wDouRF"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "imdb_dir = './AI_Tensorflow/data/aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "\n",
        "for label_type in  ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type).replace(\"\\\\\",\"/\")\n",
        "    \n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname), encoding='utf8')\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            \n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)"
      ],
      "metadata": {
        "id": "UJPxcfNOrgN7",
        "outputId": "5a75585a-ab31-4ec8-d8e9-c957dba56a48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-666d5b9231f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdir_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './AI_Tensorflow/data/aclImdb/train/neg'"
          ]
        }
      ]
    }
  ]
}